# AI 조교용 DPO 데이터셋 구축 프로젝트

GPT의 부정확한 응답을 분석하고, 학습용 `chosen/reject` 형식으로 구조화한 DPO(Direct Preference Optimization) 데이터셋을 구축한 프로젝트입니다.

---

## 기간
2024.04 ~ 2024.05 (고려대학교 / 실습 과제)

## 역할
- 1인 전담: 데이터 설계, 프롬프트 구성, 자동화 코드 작성

---

## 프로젝트 요약
- 오류 메시지 기반 학생 질문 생성
- GPT 응답 2개를 비교하여 더 적절한 응답을 `chosen`, 덜 적절한 응답을 `reject`로 분류
- 평가 기준을 수치화하고, rule-based 필터링으로 부정확한 `reject` 응답 제거

---

## 사용 기술 및 도구
- Python, HuggingFace Transformers, GPT API
- JSONL 데이터 포맷
- 프롬프트 엔지니어링
- Rule-based 필터링 스크립트 (`reject_filter.py`)

---

## 주요 기능
- 자동 질문/응답 쌍 생성 및 평가 프롬프트 구성
- 오류 유형(TypeError 등)에 따라 GPT 응답 비교
- 부정확한 응답 필터링 로직 설계
- 예시:  
  > **Q:** TypeError 관련 질문  
  > **A:** 자료형 개념 설명 (chosen)  
  > **B:** 리스트 길이 관련 설명 (reject)

---

## 배운 점
- 단순 응답 생성이 아닌 "검증 가능한 학습 데이터" 생성의 복잡성 이해
- GPT 응답의 맹신이 아닌, 명확한 기준 하의 판단 체계 설계
- 프롬프트-출력-검증 구조의 반복 최적화 경험



